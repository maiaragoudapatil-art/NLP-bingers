{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP89AE0HD9IZGwo/rnUZfUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiaragoudapatil-art/NLP-bingers/blob/main/nlp5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmDMyVkog1I5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30feb0b8-cd82-419a-b18d-81a01feab8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mxs0wwqurtRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ca3016",
        "outputId": "a07cdb91-e28a-474e-9d51-e2e03cd6f130"
      },
      "source": [
        "sample_text = \"\"\"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language. It's used in various applications like spam detection, machine translation, and sentiment analysis. For example, Google's search engine heavily relies on NLP algorithms to understand user queries and deliver relevant results. NLTK and spaCy are popular Python libraries for NLP tasks. John Doe, a prominent researcher, has contributed significantly to this area. Studying NLP allows us to build intelligent systems that can process and understand large amounts of text data efficiently and accurately.\"\"\"\n",
        "\n",
        "print(\"Sample text defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27814ac4",
        "outputId": "3bbde1c5-f7b6-4c99-ea3f-b9f7bd97e81e"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data for NLTK if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Perform sentence tokenization\n",
        "sentences_nltk = sent_tokenize(sample_text)\n",
        "\n",
        "# Perform word tokenization on the entire sample_text (or on each sentence if preferred)\n",
        "words_nltk = word_tokenize(sample_text)\n",
        "\n",
        "print(\"--- Sentence Tokenization (First 3 sentences) ---\")\n",
        "for i, sentence in enumerate(sentences_nltk[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n",
        "print(\"\\n--- Word Tokenization (First 20 words) ---\")\n",
        "print(words_nltk[:20])\n",
        "\n",
        "print(\"\\nTokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence Tokenization (First 3 sentences) ---\n",
            "Sentence 1: Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\n",
            "Sentence 2: It's used in various applications like spam detection, machine translation, and sentiment analysis.\n",
            "Sentence 3: For example, Google's search engine heavily relies on NLP algorithms to understand user queries and deliver relevant results.\n",
            "\n",
            "--- Word Tokenization (First 20 words) ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers']\n",
            "\n",
            "Tokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff99271d",
        "outputId": "d156dc9f-be0a-4288-cfb7-c326b963b7cd"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ede69770",
        "outputId": "5e1f7e56-9f7f-4cc8-f43c-cf665428ff8c"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data for NLTK if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Download 'punkt_tab' data for NLTK if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Perform sentence tokenization\n",
        "sentences_nltk = sent_tokenize(sample_text)\n",
        "\n",
        "# Perform word tokenization on the entire sample_text (or on each sentence if preferred)\n",
        "words_nltk = word_tokenize(sample_text)\n",
        "\n",
        "print(\"--- Sentence Tokenization (First 3 sentences) ---\")\n",
        "for i, sentence in enumerate(sentences_nltk[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n",
        "print(\"\\n--- Word Tokenization (First 20 words) ---\")\n",
        "print(words_nltk[:20])\n",
        "\n",
        "print(\"\\nTokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence Tokenization (First 3 sentences) ---\n",
            "Sentence 1: Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\n",
            "Sentence 2: It's used in various applications like spam detection, machine translation, and sentiment analysis.\n",
            "Sentence 3: For example, Google's search engine heavily relies on NLP algorithms to understand user queries and deliver relevant results.\n",
            "\n",
            "--- Word Tokenization (First 20 words) ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers']\n",
            "\n",
            "Tokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d797d4b7",
        "outputId": "a1562fac-2d4a-48de-c50e-7be7756cb852"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the 'punkt' tokenizer data for NLTK if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Download 'punkt_tab' data for NLTK if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Perform sentence tokenization\n",
        "sentences_nltk = sent_tokenize(sample_text)\n",
        "\n",
        "# Perform word tokenization on the entire sample_text (or on each sentence if preferred)\n",
        "words_nltk = word_tokenize(sample_text)\n",
        "\n",
        "print(\"--- Sentence Tokenization (First 3 sentences) ---\")\n",
        "for i, sentence in enumerate(sentences_nltk[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n",
        "print(\"\\n--- Word Tokenization (First 20 words) ---\")\n",
        "print(words_nltk[:20])\n",
        "\n",
        "print(\"\\nTokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence Tokenization (First 3 sentences) ---\n",
            "Sentence 1: Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\n",
            "Sentence 2: It's used in various applications like spam detection, machine translation, and sentiment analysis.\n",
            "Sentence 3: For example, Google's search engine heavily relies on NLP algorithms to understand user queries and deliver relevant results.\n",
            "\n",
            "--- Word Tokenization (First 20 words) ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers']\n",
            "\n",
            "Tokenization is the process of breaking down a text into smaller units called tokens. Sentence tokenization breaks text into sentences, and word tokenization breaks sentences into words. This is a crucial first step in NLP because it segments the raw text into manageable pieces that can then be processed and analyzed further. It helps in understanding the structure and individual components of the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a314022b",
        "outputId": "5a095b93-b68c-4ee7-f74d-5b6fca619e71"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English spaCy model (download if not present)\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the sample text with spaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Perform sentence tokenization\n",
        "sentences_spacy = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Perform word tokenization\n",
        "words_spacy = [token.text for token in doc]\n",
        "\n",
        "print(\"--- Sentence Tokenization (First 3 sentences) using spaCy ---\")\n",
        "for i, sentence in enumerate(sentences_spacy[:3]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n",
        "print(\"\\n--- Word Tokenization (First 20 words) using spaCy ---\")\n",
        "print(words_spacy[:20])\n",
        "\n",
        "print(\"\\nSpaCy's tokenization is often more advanced, providing access to linguistic features like part-of-speech tags, named entities, and dependencies right out of the box. It also handles punctuation and contractions more intelligently than simple rule-based tokenizers, making it a robust choice for deeper NLP tasks.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Sentence Tokenization (First 3 sentences) using spaCy ---\n",
            "Sentence 1: Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence that deals with the interaction between computers and human language.\n",
            "Sentence 2: It's used in various applications like spam detection, machine translation, and sentiment analysis.\n",
            "Sentence 3: For example, Google's search engine heavily relies on NLP algorithms to understand user queries and deliver relevant results.\n",
            "\n",
            "--- Word Tokenization (First 20 words) using spaCy ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers']\n",
            "\n",
            "SpaCy's tokenization is often more advanced, providing access to linguistic features like part-of-speech tags, named entities, and dependencies right out of the box. It also handles punctuation and contractions more intelligently than simple rule-based tokenizers, making it a robust choice for deeper NLP tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8df26d19",
        "outputId": "4f63a89e-c201-4160-d59a-5768e8e45a01"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure the spaCy model is loaded\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Get spaCy's default English stop words\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "# Filter words_spacy to remove stop words\n",
        "words_without_stopwords = [word.lower() for word in words_spacy if word.lower() not in stop_words]\n",
        "\n",
        "print(\"--- Words after Stop Word Removal (First 20 words) ---\")\n",
        "print(words_without_stopwords[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Words after Stop Word Removal (First 20 words) ---\n",
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'fascinating', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'computers', 'human', 'language', '.', 'applications', 'like', 'spam', 'detection']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccf41d78",
        "outputId": "73c92387-2842-43ba-f0a7-c908b232e673"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download 'punkt' if not already downloaded (necessary for some NLTK functionalities, though not directly for PorterStemmer here)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Instantiate PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Perform stemming on words_without_stopwords\n",
        "stemmed_words = [ps.stem(word) for word in words_without_stopwords]\n",
        "\n",
        "print(\"--- Words after Stemming (First 20 words) ---\")\n",
        "print(stemmed_words[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Words after Stemming (First 20 words) ---\n",
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'artifici', 'intellig', 'deal', 'interact', 'comput', 'human', 'languag', '.', 'applic', 'like', 'spam', 'detect']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "507c1ed4",
        "outputId": "226a361f-31d3-4f1a-9a60-a979d5e740e2"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure the spaCy model is loaded\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the sample text with spaCy if not already done (re-process to get a fresh doc object)\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"--- Words after Lemmatization (First 20 words) ---\")\n",
        "print(lemmatized_words[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Words after Lemmatization (First 20 words) ---\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'be', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46d0e613",
        "outputId": "23a6afb7-d89d-4691-b521-0263c0e726ee"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure the spaCy model is loaded\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the sample text with spaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(\"--- Part-of-Speech Tags (First 20 words) ---\")\n",
        "for i, (word, pos) in enumerate(pos_tags[:20]):\n",
        "    print(f\"{word}: {pos}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Part-of-Speech Tags (First 20 words) ---\n",
            "Natural: PROPN\n",
            "Language: PROPN\n",
            "Processing: PROPN\n",
            "(: PUNCT\n",
            "NLP: PROPN\n",
            "): PUNCT\n",
            "is: AUX\n",
            "a: DET\n",
            "fascinating: ADJ\n",
            "field: NOUN\n",
            "of: ADP\n",
            "Artificial: PROPN\n",
            "Intelligence: PROPN\n",
            "that: PRON\n",
            "deals: VERB\n",
            "with: ADP\n",
            "the: DET\n",
            "interaction: NOUN\n",
            "between: ADP\n",
            "computers: NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ba2f1d",
        "outputId": "c494d4e3-bb68-41ab-9340-a3b8d7e48728"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure the spaCy model is loaded\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    spacy.cli.download('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the sample text with spaCy\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "nr_entities = []\n",
        "for ent in doc.ents:\n",
        "    nr_entities.append((ent.text, ent.label_))\n",
        "\n",
        "print(\"--- Named Entities (First 10) ---\")\n",
        "for i, (entity, label) in enumerate(nr_entities[:10]):\n",
        "    print(f\"Entity: {entity}, Type: {label}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Named Entities (First 10) ---\n",
            "Entity: Natural Language Processing, Type: ORG\n",
            "Entity: NLP, Type: ORG\n",
            "Entity: Artificial Intelligence, Type: ORG\n",
            "Entity: Google, Type: ORG\n",
            "Entity: NLP, Type: ORG\n",
            "Entity: NLTK, Type: ORG\n",
            "Entity: NLP, Type: ORG\n",
            "Entity: John Doe, Type: PERSON\n",
            "Entity: NLP, Type: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IGKSCfN3uPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vnx_pdzVuZXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simple one"
      ],
      "metadata": {
        "id": "iRFE_AIquZC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (run once)\n",
        "\n",
        "# Download necessary resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Load spaCy model for POS tagging and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in California.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2. Stop Word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "print(\"After Stop Word Removal:\", filtered_tokens)\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
        "print(\"Stems:\", stems)\n",
        "\n",
        "# 4. Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "print(\"Lemmas:\", lemmas)\n",
        "\n",
        "# 5. POS Tagging (using spaCy)\n",
        "doc = nlp(text)\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# 6. Named Entity Recognition (NER)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Named Entities:\", entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSFfdnHHuQiS",
        "outputId": "c2290c7e-56ef-4d0d-9e0c-f2d7499ff0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'Steve', 'Jobs', 'founded', 'Apple', 'in', 'California', '.']\n",
            "After Stop Word Removal: ['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion', '.', 'Steve', 'Jobs', 'founded', 'Apple', 'California', '.']\n",
            "Stems: ['appl', 'look', 'buy', 'u.k.', 'startup', '$', '1', 'billion', '.', 'steve', 'job', 'found', 'appl', 'california', '.']\n",
            "Lemmas: ['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion', '.', 'Steve', 'Jobs', 'founded', 'Apple', 'California', '.']\n",
            "POS Tags: [('Apple', 'PROPN'), ('is', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('buying', 'VERB'), ('U.K.', 'PROPN'), ('startup', 'VERB'), ('for', 'ADP'), ('$', 'SYM'), ('1', 'NUM'), ('billion', 'NUM'), ('.', 'PUNCT'), ('Steve', 'PROPN'), ('Jobs', 'PROPN'), ('founded', 'VERB'), ('Apple', 'PROPN'), ('in', 'ADP'), ('California', 'PROPN'), ('.', 'PUNCT')]\n",
            "Named Entities: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY'), ('Steve Jobs', 'PERSON'), ('Apple', 'ORG'), ('California', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a8nwRHmduVo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}