{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5KqLfJN9SHzRiAuUyjnON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maiaragoudapatil-art/NLP-bingers/blob/main/nlp6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using nltk"
      ],
      "metadata": {
        "id": "XFiBH4f-iUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is an important part of Artificial Intelligence. It helps computers understand human language.\"\n"
      ],
      "metadata": {
        "id": "rImrQ4IriUoc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokaization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EGhMBnEiUmy",
        "outputId": "0708c0ae-0933-4d9e-dce6-d3c8801c76f6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Qy93zHiUhF",
        "outputId": "2411f2dc-98d8-4b50-bf2b-2f522fefe491"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'an', 'important', 'part', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normilization\n",
        "import string\n",
        "tokens_lower = [word.lower() for word in tokens]\n",
        "tokens_normalized = [word for word in tokens_lower if word not in string.punctuation]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kDH7JdT1j8U5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKSFY6JlkIu5",
        "outputId": "528a6388-7ee6-432e-c279-5571db46d8be"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'is', 'an', 'important', 'part', 'of', 'artificial', 'intelligence', 'it', 'helps', 'computers', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopword\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otGRL7S6kN78",
        "outputId": "0711d673-ed49-458d-e5d0-a2216ddb59c5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [word for word in tokens_normalized if word not in stop_words]"
      ],
      "metadata": {
        "id": "k7ZrSYFJkrgj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_no_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuiVp5BqkrVD",
        "outputId": "83413898-2cd9-45a7-b8a9-2a0034f1b949"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'important', 'part', 'artificial', 'intelligence', 'helps', 'computers', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "dvc36g9skzXX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]"
      ],
      "metadata": {
        "id": "TNxz6516k-xw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siBGSTePk-j9",
        "outputId": "e94643af-ad78-408b-d952-837c84417547"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'import', 'part', 'artifici', 'intellig', 'help', 'comput', 'understand', 'human', 'languag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loemmitization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO9KGJjqlKy-",
        "outputId": "5a4a597e-7d7f-42ab-a33e-3b28723fbc62"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykSnC4VAlZYd",
        "outputId": "c7e198fd-98d2-4918-956b-1e1b65fd81fa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'important', 'part', 'artificial', 'intelligence', 'help', 'computer', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos"
      ],
      "metadata": {
        "id": "-Pd9ZjYEmzKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing helps computers understand human language\"\n"
      ],
      "metadata": {
        "id": "4J7G9T3FmGay"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7lMKxDla0e",
        "outputId": "5d541cdf-c37f-4ad5-946e-78bcf98e3eb5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'helps', 'computers', 'understand', 'human', 'language']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odk9EVRPmi6w",
        "outputId": "1c7fe8f1-0a5a-4fb6-d169-30668ce1e937"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tags = pos_tag(tokens)\n"
      ],
      "metadata": {
        "id": "SSKO1KK3l-pV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jzcvsQl_-h",
        "outputId": "06c9d1d5-aec8-414c-db6f-c43b6fe98553"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "ZixuLBS7mpSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee05c457",
        "outputId": "138751c7-a70a-46e6-d6a7-5fcb9b727787"
      },
      "source": [
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# Download the 'maxent_ne_chunker_tab' resource if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('chunkers/maxent_ne_chunker_tab')\n",
        "except LookupError:\n",
        "    nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "# Download the 'words' corpus if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "    nltk.download('words')\n",
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "print(\"Named entities identified:\")\n",
        "print(named_entities)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities identified:\n",
            "(S\n",
            "  Natural/JJ\n",
            "  Language/NNP\n",
            "  Processing/NNP\n",
            "  helps/VBZ\n",
            "  computers/NNS\n",
            "  understand/VBP\n",
            "  human/JJ\n",
            "  language/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d768d0d",
        "outputId": "cb1f82f5-a4a7-410e-ce28-7ea03b6ba4f2"
      },
      "source": [
        "from nltk.tree import Tree\n",
        "\n",
        "print(\"Named Entities and their types:\")\n",
        "for chunk in named_entities:\n",
        "    if isinstance(chunk, Tree):\n",
        "        entity_type = chunk.label()\n",
        "        entity_text = \" \".join([word for word, tag in chunk.leaves()])\n",
        "        print(f\"  Type: {entity_type}, Entity: {entity_text}\")\n",
        "    # Optionally, print tokens that are not part of a named entity\n",
        "    # else:\n",
        "    #     print(f\"  Token: {chunk[0]}, Tag: {chunk[1]}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities and their types:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KruLbhEasTGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy"
      ],
      "metadata": {
        "id": "-e9E1YY2o76f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsd49BHqo4ud",
        "outputId": "89d50684-c9eb-4237-9b38-81df2a782f49"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.0)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "JzzYpe0Wo4kR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Elon Musk is the CEO of Tesla and SpaceX. He was born in South Africa.\"\n",
        "doc = nlp(text)\n"
      ],
      "metadata": {
        "id": "1f5mx6cEpDt1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk_KtBN4pI6b",
        "outputId": "0ae695b5-fb4e-4d5f-d012-b92c98bb6ca0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Elon', 'Musk', 'is', 'the', 'CEO', 'of', 'Tesla', 'and', 'SpaceX.', 'He', 'was', 'born', 'in', 'South', 'Africa', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normailzation\n",
        "tokens_lower = [token.text.lower() for token in doc]\n",
        "print(tokens_lower)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "921uqo3NpOyY",
        "outputId": "31baa7fa-a239-4b67-b0b5-f220fec613f6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elon', 'musk', 'is', 'the', 'ceo', 'of', 'tesla', 'and', 'spacex.', 'he', 'was', 'born', 'in', 'south', 'africa', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_clean = [token.text.lower() for token in doc\n",
        "                if not token.is_punct and not token.is_space]\n",
        "print(tokens_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiZ0r6bjpaFv",
        "outputId": "a9f56ae1-5b55-4bfe-8c36-9fed2fc004b2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['elon', 'musk', 'is', 'the', 'ceo', 'of', 'tesla', 'and', 'spacex.', 'he', 'was', 'born', 'in', 'south', 'africa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop word\n",
        "tokens_no_stopwords = [token.text for token in doc\n",
        "                       if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(tokens_no_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZqlmLGupcrf",
        "outputId": "6b7d3a15-349a-4073-891e-8793ebe3aedd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Elon', 'Musk', 'CEO', 'Tesla', 'SpaceX.', 'born', 'South', 'Africa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yt8g0-CpjCP",
        "outputId": "36ddb2e6-f635-4e8e-8b5f-8f9ce539a9e0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon → PROPN\n",
            "Musk → PROPN\n",
            "is → AUX\n",
            "the → DET\n",
            "CEO → PROPN\n",
            "of → ADP\n",
            "Tesla → PROPN\n",
            "and → CCONJ\n",
            "SpaceX. → NOUN\n",
            "He → PRON\n",
            "was → AUX\n",
            "born → VERB\n",
            "in → ADP\n",
            "South → PROPN\n",
            "Africa → PROPN\n",
            ". → PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#limitization\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypmNTe-TppZ8",
        "outputId": "04f59efb-400c-4384-9e2a-b7bd81416501"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon → Elon\n",
            "Musk → Musk\n",
            "is → be\n",
            "the → the\n",
            "CEO → CEO\n",
            "of → of\n",
            "Tesla → Tesla\n",
            "and → and\n",
            "SpaceX. → SpaceX.\n",
            "He → he\n",
            "was → be\n",
            "born → bear\n",
            "in → in\n",
            "South → South\n",
            "Africa → Africa\n",
            ". → .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ner\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"→\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtdCyjW3p0ad",
        "outputId": "b50651c1-9aaf-432c-8374-9483cd3a806f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk → PERSON\n",
            "Tesla → ORG\n",
            "South Africa → GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cWl0yclqBDG",
        "outputId": "ff27a205-9a4f-45ef-96a7-e5cc9f3621a2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk\n",
            "the CEO\n",
            "Tesla\n",
            "SpaceX.\n",
            "He\n",
            "South Africa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "seN1LgL0qKGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for regualr expression\n",
        "text = \"Contact me at email123@gmail.com on 12/05/2024!!! NLP is 100% awesome.\"\n"
      ],
      "metadata": {
        "id": "doXUqHqjrCkx"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n"
      ],
      "metadata": {
        "id": "Nc85l-79rHer"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_no_numbers = re.sub(r'\\d+', '', text)\n",
        "print(text_no_numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REAtUX6DrI3n",
        "outputId": "0aaff6bb-8d59-4682-8086-db1c7a3747c7"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contact me at email@gmail.com on //!!! NLP is % awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(text_no_punct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_DAsLyprKSb",
        "outputId": "b2c8dd9f-dd2b-4c3b-e2ed-16ed7af2f423"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contact me at email123gmailcom on 12052024 NLP is 100 awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_lower = text.lower()\n",
        "print(text_lower)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFzFQ5-srQut",
        "outputId": "ccc2ec01-fe49-4600-d006-f056305331d0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contact me at email123@gmail.com on 12/05/2024!!! nlp is 100% awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_clean_space = re.sub(r'\\s+', ' ', text)\n",
        "print(text_clean_space)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eSq7cb8rTgn",
        "outputId": "baa24f6e-40ed-47ed-b990-761ad63ce0d1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contact me at email123@gmail.com on 12/05/2024!!! NLP is 100% awesome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Numbers + Punctuation\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgzWkdKarXmY",
        "outputId": "215f8f5a-dea3-42ab-d070-aa8f11ad2bc1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contact me at emailgmailcom on  NLP is  awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acX654ddrcQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}